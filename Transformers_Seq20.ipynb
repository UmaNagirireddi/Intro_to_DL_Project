{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/indhu68/Intro_to_DL_Project/blob/main/Transformers_Seq20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qMYZVNJ_VE-C"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "data = pd.read_csv('https://raw.githubusercontent.com/indhu68/Intro_to_DL_Project/main/Kasungu_Telemetry_Pts_Oct23.csv', parse_dates=['Time.Stamp'])\n",
        "data = data[[\"Index\", \"Tag\", \"Latitude\", \"Longitude\", \"Time.Stamp\"]]\n",
        "data['Time.Stamp'] = pd.to_datetime(data['Time.Stamp'])\n",
        "data.sort_values(by=['Tag', 'Time.Stamp'], inplace=True)\n",
        "data['Time_diff'] = data['Time.Stamp'].diff().dt.total_seconds().fillna(0)\n",
        "data['Lat_diff'] = data['Latitude'].diff().fillna(0)\n",
        "data['Lon_diff'] = data['Longitude'].diff().fillna(0)\n",
        "data['Speed'] = np.sqrt(data['Lat_diff']**2 + data['Lon_diff']**2) / (data['Time_diff'] + 1e-5)\n",
        "data.fillna(method='ffill', inplace=True)\n",
        "scaler = MinMaxScaler()\n",
        "numeric_columns = ['Latitude', 'Longitude']\n",
        "data[numeric_columns] = scaler.fit_transform(data[numeric_columns])"
      ],
      "metadata": {
        "id": "CSFQZ62RaYdQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sequences(data, n_input=200, n_output=20):\n",
        "    X, y = [], []\n",
        "    for i in range(n_input, len(data) - n_output + 1):\n",
        "        X.append(data.iloc[i-n_input:i][numeric_columns].values)\n",
        "        y.append(data.iloc[i:i+n_output][['Latitude', 'Longitude']].values.flatten())\n",
        "    return np.array(X, dtype=np.float32), np.array(y, dtype=np.float32)\n",
        "\n",
        "X, y = create_sequences(data)"
      ],
      "metadata": {
        "id": "DEOpWILTaa40"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "train_dataset = TensorDataset(torch.tensor(X_train), torch.tensor(y_train))\n",
        "test_dataset = TensorDataset(torch.tensor(X_test), torch.tensor(y_test))\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
      ],
      "metadata": {
        "id": "KDvYCpBLaffE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Positional Encoding\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000, device='cuda'):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.encoding = torch.zeros(max_len, d_model, device=device)\n",
        "        position = torch.arange(0, max_len, device=device).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2, device=device).float() * -(np.log(10000.0) / d_model))\n",
        "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
        "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.encoding = self.encoding.unsqueeze(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.encoding[:, :x.size(1)]\n",
        "\n",
        "\n",
        "# Transformer Model\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, input_dim, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, output_dim, max_seq_length, device):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.embedding = nn.Linear(input_dim, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model, max_seq_length, device=device)\n",
        "        self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, batch_first=True)\n",
        "        self.fc_out = nn.Linear(d_model, output_dim)\n",
        "\n",
        "    def forward(self, src):\n",
        "        src = self.embedding(src)\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer(src, src)\n",
        "        output = self.fc_out(output[:, -1, :])\n",
        "        return output\n",
        "\n"
      ],
      "metadata": {
        "id": "59sdRkJba4lL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters and model initialization\n",
        "input_dim = 2\n",
        "d_model = 128\n",
        "nhead = 8\n",
        "num_encoder_layers = 3\n",
        "num_decoder_layers = 3\n",
        "dim_feedforward = 256\n",
        "output_dim = 40  # Adjust based on your output sequence length and dimensions\n",
        "max_seq_length = 200\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = TransformerModel(input_dim, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, output_dim, max_seq_length, device = device).to(device)\n"
      ],
      "metadata": {
        "id": "Rzq0ofcua8uv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Modified training and validation function to record losses\n",
        "def train_and_validate(model, train_loader, val_loader, num_epochs, device):\n",
        "    model.to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    best_val_loss = float('inf')\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                val_loss = criterion(outputs, labels)\n",
        "                total_val_loss += val_loss.item()\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "        train_losses.append(avg_train_loss)\n",
        "        val_losses.append(avg_val_loss)\n",
        "\n",
        "        print(f'Epoch {epoch+1}: Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "            best_val_loss = avg_val_loss\n",
        "\n",
        "    return train_losses, val_losses\n",
        "\n",
        "def plot_losses(train_losses, val_losses):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(train_losses, label='Training loss')\n",
        "    plt.plot(val_losses, label='Validation loss')\n",
        "    plt.title('Training and Validation Losses')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Training the model\n",
        "train_losses, val_losses = train_and_validate(model, train_loader, test_loader, num_epochs=50, device=device)\n",
        "\n",
        "# Plotting the losses\n",
        "plot_losses(train_losses, val_losses)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dq5UznKTa_I7",
        "outputId": "7d1d1dfc-68be-45b1-bc5d-ea4b29cc9dc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 0.0019, Validation Loss: 0.0006\n",
            "Epoch 2: Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 3: Train Loss: 0.0004, Validation Loss: 0.0005\n",
            "Epoch 4: Train Loss: 0.0004, Validation Loss: 0.0004\n",
            "Epoch 5: Train Loss: 0.0004, Validation Loss: 0.0004\n",
            "Epoch 6: Train Loss: 0.0004, Validation Loss: 0.0004\n",
            "Epoch 7: Train Loss: 0.0004, Validation Loss: 0.0004\n",
            "Epoch 8: Train Loss: 0.0003, Validation Loss: 0.0003\n",
            "Epoch 9: Train Loss: 0.0003, Validation Loss: 0.0003\n",
            "Epoch 10: Train Loss: 0.0003, Validation Loss: 0.0003\n",
            "Epoch 11: Train Loss: 0.0003, Validation Loss: 0.0003\n",
            "Epoch 12: Train Loss: 0.0003, Validation Loss: 0.0003\n",
            "Epoch 13: Train Loss: 0.0003, Validation Loss: 0.0003\n",
            "Epoch 14: Train Loss: 0.0003, Validation Loss: 0.0003\n",
            "Epoch 15: Train Loss: 0.0003, Validation Loss: 0.0003\n",
            "Epoch 16: Train Loss: 0.0002, Validation Loss: 0.0002\n",
            "Epoch 17: Train Loss: 0.0002, Validation Loss: 0.0002\n",
            "Epoch 18: Train Loss: 0.0002, Validation Loss: 0.0002\n",
            "Epoch 19: Train Loss: 0.0002, Validation Loss: 0.0002\n",
            "Epoch 20: Train Loss: 0.0002, Validation Loss: 0.0002\n",
            "Epoch 21: Train Loss: 0.0002, Validation Loss: 0.0002\n",
            "Epoch 22: Train Loss: 0.0002, Validation Loss: 0.0002\n",
            "Epoch 23: Train Loss: 0.0002, Validation Loss: 0.0002\n",
            "Epoch 24: Train Loss: 0.0002, Validation Loss: 0.0002\n",
            "Epoch 25: Train Loss: 0.0002, Validation Loss: 0.0002\n",
            "Epoch 26: Train Loss: 0.0002, Validation Loss: 0.0002\n",
            "Epoch 27: Train Loss: 0.0002, Validation Loss: 0.0002\n",
            "Epoch 28: Train Loss: 0.0002, Validation Loss: 0.0002\n",
            "Epoch 29: Train Loss: 0.0002, Validation Loss: 0.0002\n",
            "Epoch 30: Train Loss: 0.0001, Validation Loss: 0.0001\n",
            "Epoch 31: Train Loss: 0.0001, Validation Loss: 0.0001\n",
            "Epoch 32: Train Loss: 0.0001, Validation Loss: 0.0001\n",
            "Epoch 33: Train Loss: 0.0001, Validation Loss: 0.0001\n",
            "Epoch 34: Train Loss: 0.0001, Validation Loss: 0.0002\n",
            "Epoch 35: Train Loss: 0.0001, Validation Loss: 0.0001\n",
            "Epoch 36: Train Loss: 0.0001, Validation Loss: 0.0001\n",
            "Epoch 37: Train Loss: 0.0001, Validation Loss: 0.0001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader, device, scaler):\n",
        "    model.load_state_dict(torch.load('best_model.pth'))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    all_past_data = []\n",
        "    all_predictions = []\n",
        "    all_actuals = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in test_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            predictions = model(X_batch)\n",
        "            # Reshape for easier management\n",
        "            predictions = predictions.view(-1, 10, 2).cpu().numpy()\n",
        "            actuals = y_batch.view(-1, 10, 2).cpu().numpy()\n",
        "            past_data = X_batch[:, -100:, :2].view(-1, 100, 2).cpu().numpy()  # Assuming the last 100 points are relevant\n",
        "\n",
        "            # Inverse transform to original scale\n",
        "            predictions = scaler.inverse_transform(predictions.reshape(-1, 2)).reshape(-1, 10, 2)\n",
        "            actuals = scaler.inverse_transform(actuals.reshape(-1, 2)).reshape(-1, 10, 2)\n",
        "            past_data = scaler.inverse_transform(past_data.reshape(-1, 2)).reshape(-1, 100, 2)\n",
        "\n",
        "            all_past_data.append(past_data)\n",
        "            all_predictions.append(predictions)\n",
        "            all_actuals.append(actuals)\n",
        "\n",
        "    return all_past_data, all_actuals, all_predictions\n",
        "\n",
        "# Example usage\n",
        "all_past_data, all_actuals, all_predictions = evaluate_model(model, test_loader, device='cuda', scaler=scaler)\n",
        "\n"
      ],
      "metadata": {
        "id": "cpe12QrYbD3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_trajectories(past_data, actuals, predictions, index=0):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    # Plot the past trajectory\n",
        "    plt.plot(past_data[index][0][:, 0], past_data[index][0][:, 1], 'gray', label='Past Trajectory', marker='o')\n",
        "    # Plot the actual future trajectory\n",
        "    plt.plot(actuals[index][0][:, 0], actuals[index][0][:, 1], 'bo-', label='Actual Future Trajectory')\n",
        "    # Plot the predicted future trajectory\n",
        "    plt.plot(predictions[index][0][:, 0], predictions[index][0][:, 1], 'ro-', label='Predicted Future Trajectory')\n",
        "\n",
        "    plt.xlabel('Latitude')\n",
        "    plt.ylabel('Longitude')\n",
        "    plt.title('Past, Actual, and Predicted Trajectories')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "plot_trajectories(all_past_data, all_actuals, all_predictions, index=0)\n"
      ],
      "metadata": {
        "id": "fOLxDqdEbI5-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}